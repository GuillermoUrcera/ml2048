{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Machine* is our machine object. It contains a game object, which makes interfacing with the game simple. Access to game information and communication is done in the following way:\n",
    "    - The game score is **self.game.score**\n",
    "    - The game matrix is **self.game.matrix**\n",
    "    - Commanding the game is done thought the **self.act()** function. It should return:\n",
    "        - **\"'w'\"** to go up\n",
    "        - **\"'a'\"** to go left\n",
    "        - **\"'s'\"** to go right\n",
    "        - **\"'d'\"** to go down\n",
    "    - The **self.run()** function is an infinite loop that calls for a decision from the machine and tells the game to execute it and update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import puzzle\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "MAX_EPISODES=1000\n",
    "INDEX_EPISODE=0\n",
    "INDEX_EPOCH=1\n",
    "INDEX_REWARD=2\n",
    "INDEX_LOSS=3\n",
    "INDEX_SCORE=4\n",
    "INDEX_INCREMENT=5\n",
    "INDEX_LOST=6\n",
    "INDEX_LAST_STATE=7\n",
    "INDEX_ACTION=8\n",
    "INDEX_CURRENT_STATE=9\n",
    "INDEX_WEIGHTS=10\n",
    "LOG_LOCATION='../logs/log.csv'\n",
    "LOG_ITERATION=100 # Logs after this many global iterations\n",
    "#ACCESS SCORE AS self.game.score\n",
    "#ACCESS MATRIX AS self.game.matrix\n",
    "#DECIDE ACTION TO TAKE IN act()\n",
    "#POSSIBLE ACTIONS:\n",
    "#\tgo up:\t\t\"'w'\"\n",
    "#\tgo left:\t\"'a'\"\n",
    "#\tgo right:\t\"'s'\"\n",
    "#\tgo down:\t\"'d'\"\n",
    "\n",
    "class Machine:\n",
    "    game=puzzle.GameGrid() # Game object\n",
    "    epoch=0\n",
    "    episode=0\n",
    "    loss=0\n",
    "    reward=0\n",
    "    verbose_logging=False\n",
    "    weight_logging=False\n",
    "    action=\"'w'\"\n",
    "    model = Sequential()\n",
    "    inputVector=np.zeros((1, 16))\n",
    "    lastState=np.zeros((1, 16))\n",
    "    Qvalues0=np.zeros((1,4))\n",
    "    Qvalues1=np.zeros((1,4))\n",
    "    acts = [\"'w'\",\"'s'\",\"'d'\",\"'a'\"]\n",
    "    gamma = 0.9    # Discount rate\n",
    "    epsilon = 0.99  # Exploration rate\n",
    "    iteration = 0\n",
    "    dump=False\n",
    "    buffer=np.zeros((1,7))\n",
    "    bufferShape=np.zeros((1,7))\n",
    "    def __init__(self, verbose_logging_in=False,weight_logging_in=False):\n",
    "        self.dump=False\n",
    "        self.verbose_logging=verbose_logging_in\n",
    "        self.weight_logging=weight_logging_in\n",
    "        # Create model\n",
    "        self.model.add(Dense(20, input_dim=16, activation='tanh')) # Adds the first layer with 16 inputs\n",
    "        self.model.add(Dense(20, activation='tanh'))              # Adds Hidden layer with 20 nodes (Removed 'uniform')\n",
    "        self.model.add(Dense(4, activation='linear'))             # Adds output layer with 20 nodes\n",
    "        self.model.compile(loss='mse',optimizer=RMSprop(lr=0.01)) # Creates the model from all of the above\n",
    "        # Initialise log\n",
    "        with open('../logs/log.csv', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if self.verbose_logging and self.weight_logging:\n",
    "                writer.writerow([\"EPISODE\",\"EPOCH\",\"REWARD\",\"LOSS\",\"TOTAL SCORE\",\"INCREMENT IN SCORE\",\"LOST\",\"LAST STATE\",\"ACTION\",\"CURRENT STATE\",\"WEIGHTS\"]) \n",
    "                self.bufferShape=np.zeros((1,11))\n",
    "            elif self.verbose_logging:\n",
    "                writer.writerow([\"EPISODE\",\"EPOCH\",\"LAST STATE\",\"ACTION\",\"CURRENT STATE\",\"REWARD\",\"LOSS\",\"TOTAL SCORE\",\"INCREMENT IN SCORE\",\"LOST\"])\n",
    "                self.bufferShape=np.zeros((1,10))\n",
    "            else:\n",
    "                writer.writerow([\"EPISODE\",\"EPOCH\",\"REWARD\",\"LOSS\",\"TOTAL SCORE\",\"INCREMENT IN SCORE\",\"LOST\"]) \n",
    "                self.bufferShape=np.zeros((1,7))\n",
    "        self.buffer=self.bufferShape\n",
    "        self.buffer=self.buffer.tolist()\n",
    "    def run(self):\n",
    "        # Transform game state to 1D array\n",
    "        for i in range(4):\n",
    "            self.inputVector[0][0+4*i:4+4*i]=self.game.matrix[i]\n",
    "        self.epoch=self.epoch+1 # Increase epoch\n",
    "        self.iteration=self.iteration+1 # Increase global iteration\n",
    "        self.log() # Log model\n",
    "        self.game.increment=self.get_reward() # Update reward if game has been lost\n",
    "        self.game.key_down(self.act()) # Select action and update weights\n",
    "        self.lastState=self.inputVector # For logging\n",
    "        # Game loop\n",
    "        self.game.update_idletasks\n",
    "        self.game.update()\n",
    "        # pool emaG\n",
    "    def act(self):\n",
    "        if random.random() >= self.epsilon:  # Exploration\n",
    "            #print(\" Random Action \")\n",
    "            self.action = self.acts[random.randint(0,3)]\n",
    "            return self.action\n",
    "        else: \n",
    "            # Predict Q values of current state\n",
    "            self.Qvalues1[0]=self.gamma*self.model.predict(self.inputVector)+self.game.increment\n",
    "            # Extract Q value of the state\n",
    "            Q1=np.amax(self.Qvalues1)\n",
    "            Q1_index=self.Qvalues1.argmax()\n",
    "            #Construct target vector\n",
    "            self.Qvalues1=self.Qvalues0\n",
    "            self.Qvalues1[0][Q1_index]=Q1\n",
    "            # Update weights with respect to last step's prediction of this step's Q values\n",
    "            self.loss=self.model.train_on_batch(self.lastState, self.Qvalues1)\n",
    "            # Make this step's Q values next step's past Q values\n",
    "            #self.Qvalues0=self.Qvalues1\n",
    "            self.Qvalues0=self.gamma*self.model.predict(self.inputVector)+self.game.increment\n",
    "            # Select action with highest Q value\n",
    "            self.action=self.acts[self.Qvalues0.argmax()] # Don't delete this variable, it's used when logging\n",
    "            return self.action\n",
    "    def log(self):\n",
    "        # Log episode, epoch, reward, error, score, increment in score, lost, previous state, action, next state, weights \n",
    "        if self.weight_logging:\n",
    "            for layer in self.model.layers:\n",
    "                weights = layer.get_weights() # list of numpy arrays\n",
    "        if self.weight_logging and self.verbose_logging:\n",
    "            self.buffer.append([self.episode,self.epoch,self.reward,self.loss,self.game.score,self.game.increment,self.game.result,self.lastState,self.action,self.inputVector,weights])\n",
    "        elif self.verbose_logging:\n",
    "            self.buffer.append([self.episode,self.epoch,self.reward,self.loss,self.game.score,self.game.increment,self.game.result,self.lastState,self.action,self.inputVector])\n",
    "        else:\n",
    "            self.buffer.append([self.episode,self.epoch,self.reward,self.loss,self.game.score,self.game.increment,self.game.result])\n",
    "        if (self.iteration == LOG_ITERATION) or self.dump:\n",
    "            with open(LOG_LOCATION, 'a', newline='') as csvfile:\n",
    "                self.buffer=self.buffer[1:] # Remove first line of zeros               \n",
    "                writer = csv.writer(csvfile)\n",
    "                for row in self.buffer:\n",
    "                    writer.writerow(row)\n",
    "                # Reset buffer\n",
    "                self.buffer=self.bufferShape\n",
    "                self.buffer=self.buffer.tolist()\n",
    "    def dump_logs(self):\n",
    "        self.dump=True\n",
    "        self.log()\n",
    "    def plot(self):         \n",
    "        with open(LOG_LOCATION,newline='') as csvfile:\n",
    "            reader=csv.reader(csvfile)\n",
    "            # Transform reader to array\n",
    "            data=list(reader) \n",
    "            # Allocate arrays\n",
    "            x=[]\n",
    "            y=[]\n",
    "            # Get rid of labels\n",
    "            a=data.pop(0)\n",
    "            for row in data:\n",
    "                # Search for lost games\n",
    "                if row[INDEX_LOST]==\"True\": # Needed because it's read as a string and not a bool\n",
    "                    x.append(row[INDEX_EPISODE]) # Episode\n",
    "                    y.append(row[INDEX_SCORE]) # Total score     \n",
    "            # Plot results\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(x, y)\n",
    "    def get_reward(self):\n",
    "        l=0\n",
    "        if self.game.result: # If the agent lost\n",
    "            l = -2*self.game.increment-self.game.score\n",
    "            # Reset game\n",
    "            self.game.reset()\n",
    "            self.epoch=0\n",
    "            self.episode=self.episode+1\n",
    "        self.reward=self.game.increment + l # Don't delete this variable, it's used for logging\n",
    "        return (self.reward)\n",
    "        \n",
    "our_machine = Machine()\n",
    "while our_machine.episode<MAX_EPISODES:\n",
    "    our_machine.run()\n",
    "our_machine.dump_logs()\n",
    "our_machine.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
