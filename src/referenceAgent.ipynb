{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import puzzle\n",
    "\n",
    "def reward(self):\n",
    "    increment = self.game.increment\n",
    "    totalScore = self.game.totalScore\n",
    "    # loose does not work yet\n",
    "    #loose = self.game.loose\n",
    "    return increment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.gamma = 0.9    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.e_decay = .99\n",
    "        self.e_min = 0.05\n",
    "        self.learning_rate = 0.01\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=self.state_size, activation='tanh')) # Adds the first layer with 16 inputs\n",
    "        model.add(Dense(20, activation='tanh', init='uniform')) # Adds Hidden layer with 20 nodes\n",
    "        model.add(Dense(self.action_size, activation='linear')) # Adds output layer with 2 nodes\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=RMSprop(lr=self.learning_rate)) # Creates the model from all of the above\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.memory))\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        X = np.zeros((batch_size, self.state_size))\n",
    "        Y = np.zeros((batch_size, self.action_size))\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = minibatch[i]\n",
    "            target = self.model.predict(state)[0]\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target[action] = reward + self.gamma * \\\n",
    "                            np.amax(self.model.predict(next_state)[0])\n",
    "            X[i], Y[i] = state, target\n",
    "        self.model.fit(X, Y, batch_size=batch_size, nb_epoch=1, verbose=0)\n",
    "        if self.epsilon > self.e_min:\n",
    "            self.epsilon *= self.e_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Game execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import puzzle\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "MAX_EPISODES=4\n",
    "INDEX_EPISODE=0\n",
    "INDEX_EPOCH=1\n",
    "INDEX_REWARD=2\n",
    "INDEX_LOSS=3\n",
    "INDEX_SCORE=4\n",
    "INDEX_INCREMENT=5\n",
    "INDEX_LOST=6\n",
    "INDEX_LAST_STATE=7\n",
    "INDEX_ACTION=8\n",
    "INDEX_CURRENT_STATE=9\n",
    "INDEX_WEIGHTS=10\n",
    "LOG_LOCATION='../logs/log.csv'\n",
    "LOG_ITERATION=50 # Logs after this many global iterations\n",
    "#ACCESS SCORE AS self.game.score\n",
    "#ACCESS MATRIX AS self.game.matrix\n",
    "#DECIDE ACTION TO TAKE IN act()\n",
    "#POSSIBLE ACTIONS:\n",
    "#\tgo up:\t\t\"'w'\"\n",
    "#\tgo left:\t\"'a'\"\n",
    "#\tgo right:\t\"'s'\"\n",
    "#\tgo down:\t\"'d'\"\n",
    "\n",
    "class Machine:\n",
    "    game=puzzle.GameGrid() # Game object\n",
    "    epoch=0\n",
    "    episode=0\n",
    "    loss=0\n",
    "    reward=0\n",
    "    verbose_logging=False\n",
    "    weight_logging=False\n",
    "    action=\"'w'\"\n",
    "    model = Sequential()\n",
    "    inputVector=np.zeros((1, 16))\n",
    "    lastState=np.zeros((1, 16))\n",
    "    Qvalues0=np.zeros((1,4))\n",
    "    Qvalues1=np.zeros((1,4))\n",
    "    acts = [\"'w'\",\"'s'\",\"'d'\",\"'a'\"]\n",
    "    gamma = 0.9    # Discount rate\n",
    "    epsilon = 0.99  # Exploration rate\n",
    "    iteration = 0\n",
    "    dump=False\n",
    "    buffer=np.zeros((1,7))\n",
    "    bufferShape=np.zeros((1,7))\n",
    "    def __init__(self, verbose_logging_in=False,weight_logging_in=False):\n",
    "        self.dump=False\n",
    "        self.verbose_logging=verbose_logging_in\n",
    "        self.weight_logging=weight_logging_in\n",
    "        # Create model\n",
    "        self.model.add(Dense(20, input_dim=16, activation='tanh')) # Adds the first layer with 16 inputs\n",
    "        self.model.add(Dense(20, activation='tanh'))              # Adds Hidden layer with 20 nodes (Removed 'uniform')\n",
    "        self.model.add(Dense(4, activation='linear'))             # Adds output layer with 20 nodes\n",
    "        self.model.compile(loss='mse',optimizer=RMSprop(lr=0.01)) # Creates the model from all of the above\n",
    "        # Initialise log\n",
    "        with open('../logs/log.csv', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if self.verbose_logging and self.weight_logging:\n",
    "                writer.writerow([\"EPISODE\",\"EPOCH\",\"REWARD\",\"LOSS\",\"TOTAL SCORE\",\"INCREMENT IN SCORE\",\"LOST\",\"LAST STATE\",\"ACTION\",\"CURRENT STATE\",\"WEIGHTS\"]) \n",
    "                self.bufferShape=np.zeros((1,11))\n",
    "            elif self.verbose_logging:\n",
    "                writer.writerow([\"EPISODE\",\"EPOCH\",\"LAST STATE\",\"ACTION\",\"CURRENT STATE\",\"REWARD\",\"LOSS\",\"TOTAL SCORE\",\"INCREMENT IN SCORE\",\"LOST\"])\n",
    "                self.bufferShape=np.zeros((1,10))\n",
    "            else:\n",
    "                writer.writerow([\"EPISODE\",\"EPOCH\",\"REWARD\",\"LOSS\",\"TOTAL SCORE\",\"INCREMENT IN SCORE\",\"LOST\"]) \n",
    "                self.bufferShape=np.zeros((1,7))\n",
    "        self.buffer=self.bufferShape\n",
    "    def run(self):\n",
    "        # Transform game state to 1D array\n",
    "        for i in range(4):\n",
    "            self.inputVector[0][0+4*i:4+4*i]=self.game.matrix[i]\n",
    "        self.epoch=self.epoch+1 # Increase epoch\n",
    "        self.iteration=self.iteration+1 # Increase global iteration\n",
    "        self.log() # Log model\n",
    "        self.game.increment=self.get_reward() # Update reward if game has been lost\n",
    "        self.game.key_down(self.act()) # Select action and update weights\n",
    "        self.lastState=self.inputVector # For logging\n",
    "        # Game loop\n",
    "        self.game.update_idletasks\n",
    "        self.game.update()\n",
    "        # pool emaG\n",
    "    def act(self):\n",
    "        if random.random() >= self.epsilon:  # Exploration\n",
    "            #print(\" Random Action \")\n",
    "            self.action = self.acts[random.randint(0,3)]\n",
    "            return self.action\n",
    "        else: \n",
    "            # Predict Q values of current state\n",
    "            self.Qvalues1[0]=self.gamma*self.model.predict(self.inputVector)+self.game.increment\n",
    "            # Update weights with respect to last step's prediction of this step's Q values\n",
    "            self.loss=self.model.train_on_batch(self.inputVector, self.Qvalues0)\n",
    "            # Make this step's Q values next step's past Q values\n",
    "            self.Qvalues0=self.Qvalues1\n",
    "            # Select action with highest Q value\n",
    "            self.action=self.acts[self.Qvalues1.argmax()] # Don't delete this variable, it's used when logging\n",
    "            return self.action\n",
    "    def log(self):\n",
    "        # Log episode, epoch, reward, error, score, increment in score, lost, previous state, action, next state, weights \n",
    "        if self.weight_logging:\n",
    "            for layer in self.model.layers:\n",
    "                weights = layer.get_weights() # list of numpy arrays\n",
    "        if self.weight_logging and self.verbose_logging:\n",
    "            self.buffer=np.vstack([self.buffer,[self.episode,self.epoch,self.reward,self.loss,self.game.score,self.game.increment,self.game.result,self.lastState,self.action,self.inputVector,weights]])\n",
    "        elif self.verbose_logging:\n",
    "            self.buffer=np.vstack([self.buffer,[self.episode,self.epoch,self.reward,self.loss,self.game.score,self.game.increment,self.game.result,self.lastState,self.action,self.inputVector]])\n",
    "        else:\n",
    "            self.buffer=np.vstack([self.buffer,[self.episode,self.epoch,self.reward,self.loss,self.game.score,self.game.increment,self.game.result]])\n",
    "        if (self.iteration == LOG_ITERATION) or self.dump:\n",
    "            with open(LOG_LOCATION, 'a', newline='') as csvfile:\n",
    "                self.buffer=self.buffer[1:] # Remove first line of zeros               \n",
    "                writer = csv.writer(csvfile)\n",
    "                for row in self.buffer:\n",
    "                    writer.writerow(row)\n",
    "                # Reset buffer\n",
    "                self.buffer=self.bufferShape\n",
    "    def dump_logs(self):\n",
    "        self.dump=True\n",
    "        self.log()\n",
    "    def plot(self):         \n",
    "        with open(LOG_LOCATION,newline='') as csvfile:\n",
    "            reader=csv.reader(csvfile)\n",
    "            # Transform reader to array\n",
    "            data=list(reader) \n",
    "            # Allocate arrays\n",
    "            x=[]\n",
    "            y=[]\n",
    "            # Get rid of labels\n",
    "            a=data.pop(0)\n",
    "            for row in data:\n",
    "                # Search for lost games\n",
    "                if row[INDEX_LOST]==\"1.0\": # Needed because it's read as a string and not a bool\n",
    "                    x.append(row[INDEX_EPISODE]) # Episode\n",
    "                    y.append(row[INDEX_SCORE]) # Total score     \n",
    "            # Plot results\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(x, y)\n",
    "    def get_reward(self):\n",
    "        l=0\n",
    "        if self.game.result: # If the agent lost\n",
    "            l = -2*self.game.increment-self.game.score\n",
    "            # Reset game\n",
    "            self.game.reset()\n",
    "            self.epoch=0\n",
    "            self.episode=self.episode+1\n",
    "        self.reward=self.game.increment + l # Don't delete this variable, it's used for logging\n",
    "        return (self.reward)\n",
    "        \n",
    "our_machine = Machine()\n",
    "while our_machine.episode<MAX_EPISODES:\n",
    "    our_machine.run()\n",
    "our_machine.dump_logs()\n",
    "our_machine.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
